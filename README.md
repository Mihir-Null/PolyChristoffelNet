````markdown
# PolyChristoffel Network: Interpretable Dynamics via Learnable Curvatures

This project implements an **interpretable manifold dynamics model** where the learnable object is a **torsion-free connection** (Christoffel symbols) parameterized as **low-degree polynomials** of the state. Trajectories are generated by integrating the **geodesic equation**, and additional losses encourage the learned connection to be consistent with a **(pseudo-)Riemannian metric** via metric-compatibility reconstruction and **path-independence** regularization.

The implementation is **PyTorch + CUDA**, batched/parallel, with an optional jointly-trained **autoencoder** whose latent can be constrained to lie on a chosen manifold (sphere / hyperboloid / none).

---

## Quick Start

### 1) Install
```bash
pip install -r requirements.txt
````

Install PyTorch with CUDA using the official command for your platform, then install the remaining dependencies via `requirements.txt`.

### 2) Train (direct state; no autoencoder)

Default demo uses a synthetic dataset: inertial motion in Cartesian coordinates expressed in polar coordinates `(r, θ)`.

```bash
python train.py --use_ae 0 --manifold none --p 2 --q 0 --epochs 30
```

### 3) Inspect learned Christoffels

```bash
python inspect.py --ckpt checkpoints/latest.pt --use_ae 0 --manifold none --p 2 --q 0
```

Save plots if desired:

```bash
python inspect.py --ckpt checkpoints/latest.pt --save_fig figs/gamma.png
```

---

## Repository Structure

### Training / orchestration

* **`train.py`**
  Main training loop. Loads data, encodes to latent (if AE enabled), computes initial `(x0, v0)`, rolls out geodesic dynamics under learned Christoffels, computes losses (trajectory + metric/geometry regularizers), backpropagates, and saves checkpoints.

* **`config.py`**
  Central hyperparameters and switches: latent dimension `d`, polynomial degree, training schedule, loss weights, and pseudo-Riemannian signature `(p, q)` for metric validity penalties.

* **`utils.py`**
  Small utilities: seeding, device selection, and angle unwrapping (needed for polar-coordinate trajectories to avoid discontinuities at ±π).

### Data

* **`data.py`**
  Synthetic dataset for sanity-check experiments. Default: inertial Cartesian motion represented in polar coordinates `(r, θ)`; the induced Christoffels are known in closed form.

### Geometry + dynamics core

* **`poly_features.py`**
  Deterministic polynomial feature map `φ(x)` (monomial basis up to degree 2 by default). This is the interpretability anchor: Christoffels are explicit polynomials in state.

* **`christoffel.py`**
  The Christoffel network. Computes `Γ^i_{jk}(x)` as a linear combination of features `φ(x)` with learned coefficients. Enforces torsion-free symmetry (`Γ^i_{jk} = Γ^i_{kj}`) by construction.

* **`integrators.py`**
  Differentiable geodesic rollout. Integrates the second-order geodesic equation (converted to a first-order system) via a ResNet-style discrete update. Optionally applies manifold projection/retraction each step.

* **`metric_recon.py`**
  Metric reconstruction by integrating the **metric-compatibility equation** along a path, yielding a reconstructed `g(x)` given a base metric `g(x0)` and learned `Γ(x)`. Supports multi-path reconstruction to quantify path dependence (loop loss).

* **`losses.py`**
  Loss functions: trajectory loss + geometric regularizers: loop loss, metric symmetry, nondegeneracy (log|det g| barrier), fixed signature penalty (pseudo-Riemannian), coefficient weight decay.

### Optional representation learning

* **`autoencoder.py`**
  Optional MLP autoencoder trained jointly with the manifold dynamics. Latents are projected onto a chosen manifold via `manifold.py`.

* **`manifold.py`**
  Manifold constraint operators:

  * `project(z)` retraction/projection to the manifold,
  * `tangent_project(z, v)` projects velocities onto the tangent space (optional stabilization).

### Diagnostics

* **`inspect.py`**
  Evaluates learned Christoffel components on a grid and compares to ground-truth for the polar demo:

  * `Γ^r_{θθ} = -r`
  * `Γ^θ_{rθ} = Γ^θ_{θr} = 1/r`

---

## Modeled Mathematics

### 1) State, trajectories, and coordinates

Let `x(t) ∈ ℝ^d` be the state in a coordinate chart. Two modes:

* **Direct-state mode:** `x(t)` are observed coordinates.
* **Autoencoder mode:** you observe `y(t)` and set `x(t) = E(y(t))` (latent coordinates).

Goal: model time evolution as **geodesic flow** governed by a learned connection.

---

### 2) Christoffel Network: learning a torsion-free connection

#### Polynomial basis

Define monomial features `φ(x)` up to degree `p`:
[
\phi(x) = [1,; x_1,\dots,x_d,; x_1^2,; x_1x_2,\dots] \in \mathbb{R}^P.
]

#### Christoffel parameterization

Learn Christoffels as explicit polynomials:
[
\Gamma^i_{;jk}(x) = \sum_{m=1}^{P} C^i_{;jk,m},\phi_m(x),
]
with trainable coefficients (C^i_{;jk,m}).

#### Torsion-free constraint

Enforce symmetry in the lower indices:
[
\Gamma^i_{;jk}(x) = \Gamma^i_{;kj}(x).
]
This makes the connection a plausible Levi–Civita candidate and reduces degrees of freedom.

---

### 3) Dynamics: geodesic equation as an equation of motion

Geodesics satisfy:
[
\ddot{x}^i + \Gamma^i_{;jk}(x)\dot{x}^j \dot{x}^k = 0.
]

Let (v=\dot{x}). Then the first-order system is:
[
\dot{x} = v,\qquad \dot{v}^i = -\Gamma^i_{;jk}(x),v^j v^k.
]

Define acceleration:
[
a^i(x,v) = -\Gamma^i_{;jk}(x),v^j v^k.
]

**Interpretability:** acceleration is quadratic in velocity and structured entirely by (\Gamma(x)). This is a “physics-shaped” nonlinearity, not an arbitrary activation.

---

### 4) Integrator: ResNet-style discrete geodesic rollout

We discretize the geodesic ODE using a simple differentiable update:
[
v_{t+1} = v_t + \Delta t,a(x_t, v_t),\quad
x_{t+1} = x_t + \Delta t,v_{t+1}.
]

This is equivalent to stacking residual blocks that implement the geodesic flow.

---

### 5) Manifold projection / latent validity constraints (optional)

If latents must lie on a manifold (\mathcal{M}\subset\mathbb{R}^d), apply a projection/retraction:
[
x \leftarrow \Pi_{\mathcal{M}}(x).
]

Supported:

* **none**: ( \Pi(x) = x)
* **sphere**: ( \Pi(x) = x/|x|) enforcing (|x|=1)
* **hyperboloid (Lorentz model)**: enforce (\langle x,x\rangle_L = -1) with (x_0>0)

Optionally project velocity to the tangent space:

* sphere: (v \leftarrow v - \langle v,x\rangle x)
* hyperboloid: (v \leftarrow v + \langle v,x\rangle_L,x) (since (\langle x,x\rangle_L=-1))

This keeps discrete rollouts on the constraint manifold.

---

### 6) Metric reconstruction and pseudo-Riemannian consistency

A torsion-free connection (\Gamma) is not necessarily Levi–Civita for any metric. To encourage (pseudo-)Riemannian structure, reconstruct a metric (g(x)) compatible with (\Gamma(x)).

#### Metric compatibility equation

[
\partial_i g_{jk} = \Gamma^\ell_{;ij},g_{\ell k} + \Gamma^\ell_{;ik},g_{j\ell}.
]

This is a first-order linear PDE. We convert it to a path ODE by integrating along a curve (x(s)) from basepoint (x_0) to (x):
[
\frac{d}{ds} g_{jk}(s)
======================

\Big(\Gamma^\ell_{;ij}(x(s))g_{\ell k}(s) + \Gamma^\ell_{;ik}(x(s))g_{j\ell}(s)\Big)\frac{dx^i}{ds}.
]

Given:

* basepoint (x_0),
* base metric (g(x_0)=g_0) (parameterized to have fixed signature),
* a chosen path,

we integrate to get a reconstructed (g(x)).

#### Loop / path-independence loss

If (\Gamma) is not metric-realizable, reconstructed (g(x)) depends on path. We penalize this by using two paths:

* A: (x_0 \to x)
* B: (x_0 \to x_m \to x)

[
\mathcal{L}_{\text{loop}} = |g^{(A)}(x) - g^{(B)}(x)|_F^2.
]

This provides extra training signal pushing (\Gamma) toward metrizable behavior.

---

### 7) Losses / objective

#### Primary loss

* Direct-state mode:
  [
  \mathcal{L}*{\text{traj}} = \frac{1}{BT}\sum*{b,t}|\hat{x}*{b,t}-x*{b,t}|^2.
  ]

* AE mode (reconstruction in observation space):
  [
  \mathcal{L}*{\text{recon}} = \frac{1}{BT}\sum*{b,t}|D(\hat{x}*{b,t})-y*{b,t}|^2.
  ]

#### Metric validity penalties

Given reconstructed (g(x)):

* symmetry: (\mathcal{L}_{\text{sym}}=|g-g^\top|^2)
* nondegeneracy barrier: (\mathcal{L}_{\det} = \mathrm{softplus}(\alpha - \log|\det g|))
* fixed signature: penalize eigenvalues that violate desired ((p,q))

#### Simplicity penalty

Coefficient weight decay:
[
\mathcal{L}_{\text{wd}} = |C|_2^2.
]

#### Full objective

[
\mathcal{L} =
\mathcal{L}_{\text{traj/recon}}

* \lambda_{\text{loop}}\mathcal{L}_{\text{loop}}
* \lambda_{\text{sym}}\mathcal{L}_{\text{sym}}
* \lambda_{\det}\mathcal{L}_{\det}
* \lambda_{\text{sig}}\mathcal{L}_{\text{sig}}
* \lambda_{\text{wd}}\mathcal{L}_{\text{wd}}.
  ]

---

## How the pieces fit together (forward pass)

For each batch:

1. Encode (optional): (x_{0:T} = E(y_{0:T}))
2. Initialize: (x_0=x(0)), (v_0 \approx (x_1-x_0)/\Delta t)
3. Rollout: integrate geodesic dynamics under (\Gamma(x)) to get (\hat{x}_{0:T})
4. Decode (optional): (\hat{y}*{0:T} = D(\hat{x}*{0:T}))
5. Reconstruct metric (g(x)) along paths and compute loop/validity losses
6. Backprop to update (\Gamma) coefficients (and AE / base metric parameters if enabled)

---

## Notes / recommended workflow

1. Start with the polar-coordinate synthetic benchmark (no AE, SPD signature).
2. Confirm `inspect.py` shows (\Gamma^r_{\theta\theta}\approx -r) and (\Gamma^\theta_{r\theta}\approx 1/r).
3. Enable AE and then move to more complex observation models (e.g., CNN encoders).

---

## WIP

* Swap integrator for RK4 or differentiable ODE solvers (`torchdiffeq`) after pipeline validation.
* Co-train an explicit metric network (g(x)) and enforce (\Gamma_{\text{poly}}\approx \Gamma(g)) (strong Levi–Civita guarantee).
* Apply to domains with strong existing autoencoders (e.g., CNN AEs) to learn **curved latent dynamics** in concept/feature latent spaces.

```
```
